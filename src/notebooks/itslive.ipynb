{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"> \n",
    "<img src='./img/header.png'/>\n",
    "</div>\n",
    "\n",
    "## [Global Ice Velocities](https://its-live.jpl.nasa.gov/)\n",
    "    \n",
    "The Inter-mission Time Series of Land Ice Velocity and Elevation (ITS_LIVE) project facilitates ice sheet, ice shelf and glacier research by providing a globally comprehensive and temporally dense multi-sensor record of land ice velocity and elevation with low latency.\n",
    "\n",
    "Scene-pair velocities generated from satellite optical and radar imagery.\n",
    "\n",
    "* Coverage: All land ice\n",
    "* Date range: 1985-present\n",
    "* Resolution: 240m\n",
    "* Scene-pair separation: 6 to 546 days\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "* If you want to query our API directly using  your own software here is the OpenApi endpoint https://staging.nsidc.org/apps/itslive-search/docs\n",
    "* For questions about this notebook and the dataset please contact users services at uso@nsidc.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3752f954163b40d0887b7e6d571ec717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Hemisphere:', index=2, options=('global', 'south', 'north'), value='north')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204ae821c78e497caeffef39bdd7f583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Minimum % of valid pixels per image pair:'), Dropdown(layout=Layout(display='flex'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad58b9d70e04adb93accb6224c7e84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Maximum days of separation between image pairs:'), Dropdown(layout=Layout(display=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce722f6ece642488bde490483233fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectionRangeSlider(description='Date Range', index=(0, 13149), layout=Layout(width='100%'), options=((' 1984…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ab0f56910b4565808d0396368f633e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[90, 0], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', 'zoom_out_text…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1: Now let's render our UI and pick up an hemisphere, if you update the hemisphere you need to execute the cell again.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from itslive import itslive_ui\n",
    "ui = itslive_ui('north')\n",
    "ui.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current parameters: {'polygon': '-15.1749,81.3293,-15.1749,81.3313,-15.1729,81.3313,-15.1729,81.3293,-15.1749,81.3293', 'start': '1984-01-01', 'end': '2020-01-01', 'percent_valid_pixels': 1}\n",
      "Total data granules: 56,538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'count': 2, 'year': 1984},\n",
       " {'count': 129, 'year': 1985},\n",
       " {'count': 244, 'year': 1986},\n",
       " {'count': 373, 'year': 1987},\n",
       " {'count': 1890, 'year': 1988},\n",
       " {'count': 2091, 'year': 1989},\n",
       " {'count': 1455, 'year': 1990},\n",
       " {'count': 4409, 'year': 1991},\n",
       " {'count': 4401, 'year': 1992},\n",
       " {'count': 4075, 'year': 1993},\n",
       " {'count': 333, 'year': 1994},\n",
       " {'count': 1510, 'year': 1995},\n",
       " {'count': 2, 'year': 1996},\n",
       " {'count': 1285, 'year': 1997},\n",
       " {'count': 748, 'year': 1998},\n",
       " {'count': 6, 'year': 2000},\n",
       " {'count': 14, 'year': 2001},\n",
       " {'count': 5, 'year': 2002},\n",
       " {'count': 28, 'year': 2003},\n",
       " {'count': 704, 'year': 2004},\n",
       " {'count': 312, 'year': 2005},\n",
       " {'count': 1183, 'year': 2006},\n",
       " {'count': 1155, 'year': 2007},\n",
       " {'count': 550, 'year': 2008},\n",
       " {'count': 609, 'year': 2009},\n",
       " {'count': 668, 'year': 2010},\n",
       " {'count': 54, 'year': 2011},\n",
       " {'count': 51, 'year': 2012},\n",
       " {'count': 2419, 'year': 2013},\n",
       " {'count': 1296, 'year': 2014},\n",
       " {'count': 3174, 'year': 2015},\n",
       " {'count': 5226, 'year': 2016},\n",
       " {'count': 9524, 'year': 2017},\n",
       " {'count': 6613, 'year': 2018}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2: We build the parameters to query the ITS_LIVE Search API, we get the time coverage for our selected area\n",
    "params = ui.build_params()\n",
    "print(f'current parameters: {params}')\n",
    "timeline = None\n",
    "if params is not None:\n",
    "    timeline = ui.update_coverages()\n",
    "    total =  sum(item['count'] for item in timeline)\n",
    "    print(f'Total data granules: {total:,}')\n",
    "timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'http://its-live-data.jpl.nasa.gov.s3.amazonaws.com/velocity_image_pair/landsat/v00.0/32628/LT05_L1TP_016001_19850821_20170218_01_T1_X_LT05_L1GS_016246_19850602_20170219_01_T2_G0240V01_P030.nc'}\n",
      "{'url': 'http://its-live-data.jpl.nasa.gov.s3.amazonaws.com/velocity_image_pair/landsat/v00.0/32627/LT05_L1TP_003248_19890501_20170203_01_T1_X_LT05_L1TP_003248_19880412_20170209_01_T1_G0240V01_P033.nc'}\n",
      "{'url': 'http://its-live-data.jpl.nasa.gov.s3.amazonaws.com/velocity_image_pair/landsat/v00.0/32627/LT05_L1TP_003248_19900520_20170130_01_T2_X_LT05_L1TP_003248_19890501_20170203_01_T1_G0240V01_P040.nc'}\n",
      "{'url': 'http://its-live-data.jpl.nasa.gov.s3.amazonaws.com/velocity_image_pair/landsat/v00.0/32627/LT05_L1TP_003248_19900605_20170129_01_T1_X_LT05_L1TP_003248_19890501_20170203_01_T1_G0240V01_P038.nc'}\n",
      "{'url': 'http://its-live-data.jpl.nasa.gov.s3.amazonaws.com/velocity_image_pair/landsat/v00.0/32627/LT05_L1TP_003248_19920525_20180220_01_T2_X_LT05_L1TP_017001_19920425_20180220_01_T2_G0240V01_P078.nc'}\n",
      "{'url': 'http://its-live-data.jpl.nasa.gov.s3.amazonaws.com/velocity_image_pair/landsat/v00.0/32627/LT05_L1TP_003248_19920610_20180220_01_T2_X_LT05_L1TP_017001_19920425_20180220_01_T2_G0240V01_P075.nc'}\n",
      "{'url': 'http://its-live-data.jpl.nasa.gov.s3.amazonaws.com/velocity_image_pair/landsat/v00.0/32627/LT05_L1TP_003248_19930613_20180220_01_T2_X_LT05_L1TP_016001_19930405_20180220_01_T2_G0240V01_P064.nc'}\n",
      "{'url': 'http://its-live-data.jpl.nasa.gov.s3.amazonaws.com/velocity_image_pair/landsat/v00.0/32627/LT05_L1TP_003248_19930613_20180220_01_T2_X_LT05_L1TP_017001_19930327_20180220_01_T2_G0240V01_P075.nc'}\n",
      "{'url': 'http://its-live-data.jpl.nasa.gov.s3.amazonaws.com/velocity_image_pair/landsat/v00.0/32627/LT05_L1TP_003248_19930715_20180228_01_T1_X_LT05_L1TP_015001_19930430_20180220_01_T2_G0240V01_P045.nc'}\n",
      "{'url': 'http://its-live-data.jpl.nasa.gov.s3.amazonaws.com/velocity_image_pair/landsat/v00.0/32627/LT05_L1TP_003248_19930715_20180228_01_T1_X_LT05_L1TP_016001_19930421_20180220_01_T2_G0240V01_P016.nc'}\n"
     ]
    }
   ],
   "source": [
    "#3: Now we are going to get the velocity pair urls, this does not download the files yet just their location\n",
    "urls = []\n",
    "params = ui.build_params()\n",
    "if params is not None:\n",
    "    urls = ui.get_granule_urls(params)\n",
    "    # Print the first 10 granule URLs\n",
    "    for url in urls[0:10]:\n",
    "        print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering granules by year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4: This will query AWS(where the granules are stored) so we know the total size of our first N granules\n",
    "# This may take some time, try reducing the selected area or constraining the other parameters to download a reasonable number of granules.\n",
    "url_list = [url['url'] for url in urls]\n",
    "# urls\n",
    "filtered_urls = ui.filter_urls(url_list, max_files_per_year=5, months=[3,4,5,6,7])\n",
    "\n",
    "# max_granules = 100\n",
    "# sizes = ui.calculate_file_sizes(filtered_urls, max_granules)\n",
    "# total_zise = round(sum(sizes)/1024,2)\n",
    "# print(f'Approx size to download for the first {max_granules:,} granules: {total_zise} MB')\n",
    "print(len(filtered_urls))\n",
    "filtered_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "**Now that we have our list of data granules we can download them from AWS.**\n",
    "\n",
    "If this notebook is running inside AWS we could load the granules into a Dask cluster and reduce our processing times and costs.\n",
    "Let's get some coffee, some data requests are in the Gigabytes realm and may take a little while to be processed. \n",
    "Once that your status URL says is completed we can grab the HDF5 data file using the URL on the same response!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 This will download the first 50 velocity pairs\n",
    "files = ui.download_velocity_pairs(filtered_urls, start=0, end=50)\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import pyproj\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from shapely.geometry import Polygon\n",
    "warnings.filterwarnings('ignore')\n",
    "centroid = ui.dc.last_draw['geometry']['coordinates']\n",
    "# coord = [-49.59321, 69.210579]\n",
    "#loads an array of xarray datasets from the nc files\n",
    "velocity_pairs = ui.load_velocity_pairs('data')\n",
    "\n",
    "centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocities = []\n",
    "mean_offset_meters = 1200\n",
    "for ds in velocity_pairs:\n",
    "    proj = str(int(ds.UTM_Projection.spatial_epsg))\n",
    "    selected_coord = ui.transform_coord('4326',proj, centroid[0], centroid[1])\n",
    "    projected_lon = round(selected_coord[0])\n",
    "    projected_lat = round(selected_coord[1])\n",
    "    mid_date = datetime.strptime(ds.img_pair_info.date_center,'%Y%m%d')\n",
    "    # We are going to calculate the mean value of the neighboring pixels(each is 240m) 10 x 10 window\n",
    "    mask_lon = (ds.x >= projected_lon - mean_offset_meters) & (ds.x <= projected_lon + mean_offset_meters)\n",
    "    mask_lat = (ds.y >= projected_lat - mean_offset_meters) & (ds.y <= projected_lat + mean_offset_meters)\n",
    "    v = ds.where(mask_lon & mask_lat , drop=True).v.mean(skipna=True)\n",
    "    # If we have a valid value we add it to the velocities array.\n",
    "    if not np.isnan(v):\n",
    "        velocities.append({'date': mid_date, 'mean_velocity': v.values.ravel()[0]})\n",
    "velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# order by date\n",
    "df = pd.DataFrame(velocities)\n",
    "df = df.sort_values(by='date').reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x ='date', y='mean_velocity', kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_pairs[0].plot.scatter(x='x', y='y', hue='v')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
